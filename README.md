# ðŸ§  LaySumm: Research on Simplified Summarization with LLMs

[![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)  
**LaySumm** is a research project focused on generating **Plain Language Summaries (PLS)** from complex documents (e.g., medical, scientific, or legal) using Large Language Models (LLMs) like GPT-4, LLaMA, Mistral, and others. This project explores **prompt engineering**, **fine-tuning**, and **retrieval-augmented generation (RAG)** strategies to produce summaries that are **readable**, **factually accurate**, and **accessible** to general audiences.

---

## ðŸŽ¯ Project Goals

- âœ… Build reproducible pipelines for generating simplified summaries using LLMs.
- âœ… Evaluate readability (e.g., Flesch-Kincaid) and factual consistency of generated content.
- âœ… Compare OpenAI, Hugging Face, and open-source models for PLS tasks.
- âœ… Explore prompt-tuning, fine-tuning, and retrieval-based summarization.
- âœ… Provide dashboards and metrics to visualize improvement over baselines.

---

## ðŸ§± Repository Structure

```bash
LaySumm/
â”œâ”€â”€ data/                  # Input and output datasets
â”‚   â”œâ”€â”€ raw/               # Raw input documents
â”‚   â”œâ”€â”€ processed/         # Preprocessed input/output pairs
â”‚   â”œâ”€â”€ prompts/           # Prompt templates
â”‚   â””â”€â”€ datasets.md        # Documentation for datasets
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ baselines/         # Prompt-only summarizers (e.g., GPT-4)
â”‚   â”œâ”€â”€ fine-tuned/        # Fine-tuned checkpoints
â”‚   â””â”€â”€ retriever/         # RAG components (e.g., FAISS, Azure Search)
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ metrics.py         # ROUGE, BERTScore, readability metrics
â”‚   â”œâ”€â”€ factual_eval.py    # QA-based or LLM-based factuality scoring
â”‚   â”œâ”€â”€ results/           # Evaluation output data
â”‚   â””â”€â”€ plots/             # Visualization (charts, graphs)
â”‚
â”œâ”€â”€ notebooks/             # Jupyter Notebooks for experimentation
â”‚   â”œâ”€â”€ 01_generate_summary.ipynb
â”‚   â”œâ”€â”€ 02_fine_tuning.ipynb
â”‚   â””â”€â”€ 03_evaluation_dashboard.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipeline.py        # End-to-end summarization pipeline
â”‚   â”œâ”€â”€ inference.py       # Model runner
â”‚   â””â”€â”€ retriever.py       # For RAG use case
â”‚
â”œâ”€â”€ docs/                  # Project documentation
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ evaluation-methods.md
â”‚   â””â”€â”€ model-notes.md
â”‚
â”œâ”€â”€ research-paper.md      # Extended summary of research or whitepaper
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ LICENSE                # MIT License
â”œâ”€â”€ .gitignore             # Ignore files (e.g., checkpoints, env)
â””â”€â”€ README.md              # You're here!
